---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app minio
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
    namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    controllers:
      minio:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: quay.io/minio/minio
              tag: RELEASE.2025-04-22T22-12-26Z # latest version of Minio with complete console! (TrueChart v1.2.23)
              #tag: RELEASE.2025-01-20T14-49-07Z # latest version TrueNAS Core 13.3
              #tag: RELEASE.2025-06-13T11-33-47Z # latest version TrueNAS SCALE 25.04.1 (TrueChart v1.3.4)
            env:
              TZ: "${TIMEZONE}"
              MINIO_UPDATE: "off"
              MINIO_BROWSER_REDIRECT_URL: https://minio.${SECRET_DOMAIN}
              MINIO_SERVER_URL: https://s3.${SECRET_DOMAIN}
              MINIO_API_CORS_ALLOW_ORIGIN: https://minio.${SECRET_DOMAIN},https://s3.${SECRET_DOMAIN}
              #MINIO_PROMETHEUS_URL: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
              #MINIO_PROMETHEUS_JOB_ID: minio-job
              #MINIO_PROMETHEUS_AUTH_TYPE: public
            envFrom:
              - secretRef:
                  name: *app
            args: ["server", "/data", "--console-address", ":9001"]
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /minio/health/live
                    port: &api-port 9000
                  initialDelaySeconds: 0
                  periodSeconds: 10
                  timeoutSeconds: 1
                  failureThreshold: 3
              readiness: *probes
            resources:
              requests:
                memory: 100Mi
                cpu: 50m
              limits:
                memory: 750Mi
        pod:
          securityContext:
            runAsUser: 473
            runAsGroup: 473
            fsGroup: 473
            fsGroupChangePolicy: OnRootMismatch
    service:
      app:
        controller: minio
        ports:
          http:
            primary: true
            port: 9001
          api:
            port: *api-port
    serviceMonitor:
      main:
        serviceName: *app
        enabled: false
        endpoints:
          - port: api
            scheme: http
            path: /minio/v2/metrics/cluster
            interval: 1m
            scrapeTimeout: 10s
            bearerTokenSecret:
              name: minio
              key: MINIO_PROMETHEUS_TOKEN
    ingress:
      app:
        className: internal
        hosts:
          - host: &minioHost "minio.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *minioHost
      s3:
        className: internal
        annotations:
          nginx.ingress.kubernetes.io/proxy-connect-timeout: "180"
          nginx.ingress.kubernetes.io/proxy-body-size: 1024m
          nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
          nginx.ingress.kubernetes.io/configuration-snippet: |
            chunked_transfer_encoding off;
        hosts:
          - host: &s3Host s3.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: app
                  port: api
        tls:
          - hosts:
              - *s3Host
    persistence:
      data:
        enabled: true
        existingClaim: nas-minio
